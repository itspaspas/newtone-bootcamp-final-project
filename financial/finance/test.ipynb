{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5e2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itspa\\Downloads\\Personal Projects\\newtone-bootcamp-final-project\\financial\\finance\\td3_agent.py:137: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  noise = torch.FloatTensor([self.noise.sample() for _ in range(len(actions))]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100/5000]\tAverage Shortfall: $298,939,184.95\n",
      "Episode [200/5000]\tAverage Shortfall: $291,942,207.16\n",
      "Episode [300/5000]\tAverage Shortfall: $217,240,898.41\n",
      "Episode [400/5000]\tAverage Shortfall: $170,565,051.72\n",
      "Episode [500/5000]\tAverage Shortfall: $167,340,987.96\n",
      "Episode [600/5000]\tAverage Shortfall: $42,774,037.28\n",
      "Episode [700/5000]\tAverage Shortfall: $635,360.88\n",
      "Episode [800/5000]\tAverage Shortfall: $687,861.72\n",
      "Episode [900/5000]\tAverage Shortfall: $667,931.85\n",
      "Episode [1000/5000]\tAverage Shortfall: $579,304.33\n",
      "Episode [1100/5000]\tAverage Shortfall: $572,600.37\n",
      "Episode [1200/5000]\tAverage Shortfall: $617,773.53\n",
      "Episode [1300/5000]\tAverage Shortfall: $604,536.35\n",
      "Episode [1400/5000]\tAverage Shortfall: $685,579.13\n",
      "Episode [1500/5000]\tAverage Shortfall: $669,446.19\n",
      "Episode [1600/5000]\tAverage Shortfall: $599,104.82\n",
      "Episode [1700/5000]\tAverage Shortfall: $700,718.55\n",
      "Episode [1800/5000]\tAverage Shortfall: $598,708.84\n",
      "Episode [1900/5000]\tAverage Shortfall: $663,808.95\n",
      "Episode [2000/5000]\tAverage Shortfall: $640,402.67\n",
      "Episode [2100/5000]\tAverage Shortfall: $639,197.60\n",
      "Episode [2200/5000]\tAverage Shortfall: $7,755,563.87\n",
      "Episode [2300/5000]\tAverage Shortfall: $7,451,437.28\n",
      "Episode [2400/5000]\tAverage Shortfall: $617,947.93\n",
      "Episode [2500/5000]\tAverage Shortfall: $710,246.03\n",
      "Episode [2600/5000]\tAverage Shortfall: $621,636.23\n",
      "Episode [2700/5000]\tAverage Shortfall: $703,651.89\n",
      "Episode [2800/5000]\tAverage Shortfall: $722,904.58\n",
      "Episode [2900/5000]\tAverage Shortfall: $613,415.80\n",
      "Episode [3000/5000]\tAverage Shortfall: $627,306.59\n",
      "Episode [3100/5000]\tAverage Shortfall: $662,682.90\n",
      "Episode [3200/5000]\tAverage Shortfall: $661,695.82\n",
      "Episode [3300/5000]\tAverage Shortfall: $611,644.62\n",
      "Episode [3400/5000]\tAverage Shortfall: $626,134.04\n",
      "Episode [3500/5000]\tAverage Shortfall: $617,678.65\n",
      "Episode [3600/5000]\tAverage Shortfall: $630,390.05\n",
      "Episode [3700/5000]\tAverage Shortfall: $697,925.57\n",
      "Episode [3800/5000]\tAverage Shortfall: $636,659.14\n",
      "Episode [3900/5000]\tAverage Shortfall: $643,374.50\n",
      "Episode [4000/5000]\tAverage Shortfall: $720,982.35\n",
      "Episode [4100/5000]\tAverage Shortfall: $653,346.53\n",
      "Episode [4200/5000]\tAverage Shortfall: $596,098.52\n",
      "Episode [4300/5000]\tAverage Shortfall: $656,544.28\n",
      "Episode [4400/5000]\tAverage Shortfall: $645,576.08\n",
      "Episode [4500/5000]\tAverage Shortfall: $692,044.59\n",
      "Episode [4600/5000]\tAverage Shortfall: $681,515.36\n",
      "Episode [4700/5000]\tAverage Shortfall: $666,170.65\n",
      "Episode [4800/5000]\tAverage Shortfall: $742,548.72\n",
      "Episode [4900/5000]\tAverage Shortfall: $633,887.50\n",
      "Episode [5000/5000]\tAverage Shortfall: $588,005.67\n",
      "\n",
      "Average Implementation Shortfall: $24,625,074.38 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "import td3_agent\n",
    "from td3_agent import TD3\n",
    "import utils\n",
    "import importlib\n",
    "import price_models\n",
    "import rewards as rw\n",
    "importlib.reload(rw)\n",
    "importlib.reload(price_models)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(sca)\n",
    "importlib.reload(td3_agent)\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "\n",
    "# Create simulation environment\n",
    "rf = rw.CjOeCriterion(\n",
    "    per_step_inventory_aversion=0.01,\n",
    "    terminal_inventory_aversion=0.0,\n",
    "    inventory_exponent=2.0,\n",
    "    terminal_time=1.0\n",
    ")\n",
    "\n",
    "env = sca.MarketEnvironment(reward_function=rf)\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = TD3(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 5000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "\n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f8a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, episodes=100):\n",
    "    shortfalls = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset(ep)\n",
    "        env.start_transactions()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, = agent.act(state, add_noise=False)\n",
    "            state, _, done, info = env.step(action)\n",
    "        shortfalls.append(info.implementation_shortfall)\n",
    "    return np.array(shortfalls)\n",
    "\n",
    "def evaluate_ac(env, episodes=100):\n",
    "    frac_schedule = env.get_trade_list() / env.total_shares  \n",
    "    shortfalls = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset(ep)\n",
    "        env.start_transactions()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            action = frac_schedule[t]  \n",
    "            state, _, done, info = env.step(action)\n",
    "            t += 1\n",
    "        shortfalls.append(info.implementation_shortfall)\n",
    "    return np.array(shortfalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffa05d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: mean=$666096.83, std=$410591.73\n",
      "AC   : mean=$1732920.49, std=$2359937.15\n"
     ]
    }
   ],
   "source": [
    "agent_sfs = evaluate_agent(env, agent, episodes=1000)\n",
    "ac_sfs    = evaluate_ac(env, episodes=1000)\n",
    "\n",
    "print(\"Agent: mean=${:.2f}, std=${:.2f}\".format(agent_sfs.mean(), agent_sfs.std()))\n",
    "print(\"AC   : mean=${:.2f}, std=${:.2f}\".format(ac_sfs.mean(),    ac_sfs.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
