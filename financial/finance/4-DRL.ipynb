{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Optimal Execution of Portfolio Transactions     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use Deep Reinforcement Learning (DRL) for optimizing the execution of large portfolio transactions. We begin with a brief review of reinforcement learning and actor-critic methods.  Then, you will use an actor-critic method to generate optimal trading strategies that maximize profit when liquidating a block of shares. \n",
    "\n",
    "# Actor-Critic Methods\n",
    "\n",
    "In reinforcement learning, an agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act in a way that will maximize its expected long-term rewards. \n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./text_images/RL.png\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 1. - Reinforcement Learning.</figcaption>\n",
    "</figure> \n",
    "<br>\n",
    "\n",
    "There are several types of RL algorithms, and they can be divided into three groups:\n",
    "\n",
    "- **Critic-Only**: Critic-Only methods, also known as Value-Based methods, first find the optimal value function and then derive an optimal policy from it. \n",
    "\n",
    "\n",
    "- **Actor-Only**: Actor-Only methods, also known as Policy-Based methods, search directly for the optimal policy in policy space. This is typically done by using a parameterized family of policies over which optimization procedures can be used directly. \n",
    "\n",
    "\n",
    "- **Actor-Critic**: Actor-Critic methods combine the advantages of actor-only and critic-only methods. In this method, the critic learns the value function and uses it to determine how the actor's policy parramerters should be changed. In this case, the actor brings the advantage of computing continuous actions without the need for optimization procedures on a value function, while the critic supplies the actor with knowledge of the performance. Actor-critic methods usually have good convergence properties, in contrast to critic-only methods.  The **Deep Deterministic Policy Gradients (DDPG)** algorithm is one example of an actor-critic method.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./text_images/Actor-Critic.png\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 2. - Actor-Critic Reinforcement Learning.</figcaption>\n",
    "</figure> \n",
    "<br>\n",
    "\n",
    "In this notebook, you will use DDPG to determine the optimal execution of portfolio transactions. In other words, you will use the DDPG algorithm to solve the optimal liquidation problem. But before you can apply the DDPG algorithm we first need to formulate the optimal liquidation problem so that in can be solved using reinforcement learning. In the next section we will see how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Optimal Execution as a Reinforcement Learning Problem\n",
    "\n",
    "As we learned in the previous notebooks, the optimal liquidation problem is a minimization problem, *i.e.* we need to find the trading list that minimizes the implementation shortfall. In order to solve this problem through reinforcement learning, we need to restate the optimal liquidation problem in terms of **States**, **Actions**, and **Rewards**. Let's start by defining our States.\n",
    "\n",
    "### States\n",
    "\n",
    "The optimal liquidation problem entails that we sell all our shares within a given time frame. Therefore, our state vector must contain some information about the time remaining, or what is equivalent, the number trades remaning. We will use the latter and use the following features to define the state vector at time $t_k$:\n",
    "\n",
    "\n",
    "$$\n",
    "[r_{k-5},\\, r_{k-4},\\, r_{k-3},\\, r_{k-2},\\, r_{k-1},\\, r_{k},\\, m_{k},\\, i_{k}]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $r_{k} = \\log\\left(\\frac{\\tilde{S}_k}{\\tilde{S}_{k-1}}\\right)$ is the log-return at time $t_k$\n",
    "\n",
    "\n",
    "- $m_{k} = \\frac{N_k}{N}$ is the number of trades remaining at time $t_k$ normalized by the total number of trades.\n",
    "\n",
    "\n",
    "- $i_{k} = \\frac{x_k}{X}$ is the remaining number of shares at time $t_k$ normalized by the total number of shares.\n",
    "\n",
    "The log-returns capture information about stock prices before time $t_k$, which can be used to detect possible price trends. The number of trades and shares remaining allow the agent to learn to sell all the shares within a given time frame. It is important to note that in real world trading scenarios, this state vector can hold many more variables. \n",
    "\n",
    "### Actions\n",
    "\n",
    "Since the optimal liquidation problem only requires us to sell stocks, it is reasonable to define the action $a_k$ to be the number of shares to sell at time $t_{k}$. However, if we start with millions of stocks, intepreting the action directly as the number of shares to sell at each time step can lead to convergence problems, because, the agent will need to produce actions with very high values. Instead, we will interpret the action $a_k$ as a **percentage**. In this case, the actions produced by the agent will only need to be between 0 and 1. Using this interpretation, we can determine the number of shares to sell at each time step using:\n",
    "\n",
    "$$\n",
    "n_k = a_k \\times x_k\n",
    "$$\n",
    "\n",
    "where $x_k$ is the number of shares remaining at time $t_k$.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "Defining the rewards is trickier than defining states and actions, since the original problem is a minimization problem. One option is to use the difference between two consecutive utility functions. Remeber the utility function is given by:\n",
    "\n",
    "$$\n",
    "U(x) = E(x) + λ V(x)\n",
    "$$\n",
    "\n",
    "After each time step, we compute the **utility using the equations for $E(x)$ and $V(x)$ from the Almgren and Chriss model for the remaining time and inventory while holding parameter λ constant.** Denoting the optimal trading trajectory computed at time $t$ as $x^*_t$, we define the reward as: \n",
    "\n",
    "$$\n",
    "R_{t} = {{U_t(x^*_t) - U_{t+1}(x^*_{t+1})}\\over{U_t(x^*_t)}}\n",
    "$$\n",
    "\n",
    "Where we have normalized the difference to train the actor-critic model easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment\n",
    "\n",
    "In order to train our DDPG algorithm we will use a very simple simulated trading environment. This environment simulates stock prices that follow a discrete arithmetic random walk and that the permanent and temporary market impact functions are linear functions of the rate of trading, just like in the Almgren and Chriss model. This simple trading environment serves as a starting point to create more complex trading environments. You are encouraged to extend this simple trading environment by adding more complexity to simulte real world trading dynamics, such as book orders, network latencies, trading fees, etc... \n",
    "\n",
    "The simulated enviroment is contained in the **syntheticChrissAlmgren.py** module. You are encouraged to take a look it and modify its parameters as you wish. Let's take a look at the default parameters of our simulation environment. We have set the intial stock price to be $S_0 = 50$, and the total number of shares to sell to one million. This gives an initial portfolio value of 50 Million dollars. We have also set the trader's risk aversion to $\\lambda = 10^{-6}$.\n",
    "\n",
    "The stock price will have 12\\% annual volatility, a [bid-ask spread](https://www.investopedia.com/terms/b/bid-askspread.asp) of 1/8 and an average daily trading volume of 5 million shares. Assuming there are 250 trading days in a year, this gives a daily volatility in stock price of $0.12 / \\sqrt{250} \\approx 0.8\\%$. We will use a liquiditation time of $T = 60$ days and we will set the number of trades $N = 60$. This means that $\\tau=\\frac{T}{N} = 1$ which means we will be making one trade per day. \n",
    "\n",
    "For the temporary cost function we will set the fixed cost of selling to be 1/2 of the bid-ask spread, $\\epsilon = 1/16$. we will set $\\eta$ such that for each one percent of the daily volume we trade, we incur a price impact equal to the bid-ask\n",
    "spread. For example, trading at a rate of $5\\%$ of the daily trading volume incurs a one-time cost on each trade of 5/8. Under this assumption we have $\\eta =(1/8)/(0.01 \\times 5 \\times 10^6) = 2.5 \\times 10^{-6}$.\n",
    "\n",
    "For the permanent costs, a common rule of thumb is that price effects become significant when we sell $10\\%$ of the daily volume. If we suppose that significant means that the price depression is one bid-ask spread, and that the effect is linear for smaller and larger trading rates, then we have $\\gamma = (1/8)/(0.1 \\times 5 \\times 10^6) = 2.5 \\times 10^{-7}$. \n",
    "\n",
    "The tables below summarize the default parameters of the simulation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Get the default financial and AC Model parameters\n",
    "financial_params, ac_params = utils.get_env_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Financial Parameters</caption>\n",
       "<tr>\n",
       "  <th>Annual Volatility:</th>  <td>12%</td> <th>  Bid-Ask Spread:    </th>     <td>0.125</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Daily Volatility:</th>  <td>0.8%</td> <th>  Daily Trading Volume:</th> <td>5,000,000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Annual Volatility:} &  12\\% & \\textbf{  Bid-Ask Spread:    }   &   0.125    \\\\\n",
       "\\textbf{Daily Volatility:}  & 0.8\\% & \\textbf{  Daily Trading Volume:} & 5,000,000  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Financial Parameters}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Almgren and Chriss Model Parameters</caption>\n",
       "<tr>\n",
       "  <th>Total Number of Shares to Sell:</th>                  <td>1,000,000</td> <th>  Fixed Cost of Selling per Share:</th> <td>$0.062</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Starting Price per Share:</th>                         <td>$50.00</td>   <th>  Trader's Risk Aversion:</th>           <td>1e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Price Impact for Each 1% of Daily Volume Traded:</th> <td>$2.5e-06</td>  <th>  Permanent Impact Constant:</th>       <td>2.5e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Number of Days to Sell All the Shares:</th>              <td>60</td>     <th>  Single Step Variance:</th>             <td>0.144</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Number of Trades:</th>                                   <td>60</td>     <th>  Time Interval between trades:</th>      <td>1.0</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Total Number of Shares to Sell:}                   & 1,000,000 & \\textbf{  Fixed Cost of Selling per Share:} & \\$0.062  \\\\\n",
       "\\textbf{Starting Price per Share:}                         &  \\$50.00  & \\textbf{  Trader's Risk Aversion:}          &  1e-06   \\\\\n",
       "\\textbf{Price Impact for Each 1\\% of Daily Volume Traded:} & \\$2.5e-06 & \\textbf{  Permanent Impact Constant:}       & 2.5e-07  \\\\\n",
       "\\textbf{Number of Days to Sell All the Shares:}            &     60    & \\textbf{  Single Step Variance:}            &  0.144   \\\\\n",
       "\\textbf{Number of Trades:}                                 &     60    & \\textbf{  Time Interval between trades:}    &   1.0    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Almgren and Chriss Model Parameters}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "In the code below we use **DDPG** to find a policy that can generate optimal trading trajectories that minimize implementation shortfall, and can be benchmarked against the Almgren and Chriss model. We will implement a typical reinforcement learning workflow to train the actor and critic using the simulation environment. We feed the states observed from our simulator to an agent. The Agent first predicts an action using the actor model and performs the action in the environment. Then, environment returns the reward and new state. This process continues for the given number of episodes. To get accurate results, you should run the code at least 10,000 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with reward function of $$ r = Q_t (P - P_0) - \\alpha d_t $$ but tweaked a little bit made convex and some other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100/10000]\tAverage Shortfall: $1,115,524.99\n",
      "Episode [200/10000]\tAverage Shortfall: $736,189.15\n",
      "Episode [300/10000]\tAverage Shortfall: $653,020.01\n",
      "Episode [400/10000]\tAverage Shortfall: $657,455.83\n",
      "Episode [500/10000]\tAverage Shortfall: $713,015.27\n",
      "Episode [600/10000]\tAverage Shortfall: $642,479.18\n",
      "Episode [700/10000]\tAverage Shortfall: $662,310.81\n",
      "Episode [800/10000]\tAverage Shortfall: $669,257.31\n",
      "Episode [900/10000]\tAverage Shortfall: $671,740.87\n",
      "Episode [1000/10000]\tAverage Shortfall: $659,986.13\n",
      "Episode [1100/10000]\tAverage Shortfall: $663,747.48\n",
      "Episode [1200/10000]\tAverage Shortfall: $666,158.99\n",
      "Episode [1300/10000]\tAverage Shortfall: $621,277.47\n",
      "Episode [1400/10000]\tAverage Shortfall: $643,741.79\n",
      "Episode [1500/10000]\tAverage Shortfall: $680,184.38\n",
      "Episode [1600/10000]\tAverage Shortfall: $685,300.58\n",
      "Episode [1700/10000]\tAverage Shortfall: $721,398.55\n",
      "Episode [1800/10000]\tAverage Shortfall: $674,768.55\n",
      "Episode [1900/10000]\tAverage Shortfall: $671,332.48\n",
      "Episode [2000/10000]\tAverage Shortfall: $683,757.23\n",
      "Episode [2100/10000]\tAverage Shortfall: $608,460.90\n",
      "Episode [2200/10000]\tAverage Shortfall: $630,664.00\n",
      "Episode [2300/10000]\tAverage Shortfall: $663,729.88\n",
      "Episode [2400/10000]\tAverage Shortfall: $671,909.57\n",
      "Episode [2500/10000]\tAverage Shortfall: $591,556.20\n",
      "Episode [2600/10000]\tAverage Shortfall: $701,376.68\n",
      "Episode [2700/10000]\tAverage Shortfall: $663,724.92\n",
      "Episode [2800/10000]\tAverage Shortfall: $655,136.50\n",
      "Episode [2900/10000]\tAverage Shortfall: $659,788.76\n",
      "Episode [3000/10000]\tAverage Shortfall: $646,975.91\n",
      "Episode [3100/10000]\tAverage Shortfall: $640,154.93\n",
      "Episode [3200/10000]\tAverage Shortfall: $650,621.20\n",
      "Episode [3300/10000]\tAverage Shortfall: $694,887.65\n",
      "Episode [3400/10000]\tAverage Shortfall: $721,790.54\n",
      "Episode [3500/10000]\tAverage Shortfall: $623,096.97\n",
      "Episode [3600/10000]\tAverage Shortfall: $663,852.82\n",
      "Episode [3700/10000]\tAverage Shortfall: $636,976.71\n",
      "Episode [3800/10000]\tAverage Shortfall: $585,254.08\n",
      "Episode [3900/10000]\tAverage Shortfall: $616,676.65\n",
      "Episode [4000/10000]\tAverage Shortfall: $615,902.16\n",
      "Episode [4100/10000]\tAverage Shortfall: $718,621.12\n",
      "Episode [4200/10000]\tAverage Shortfall: $684,708.62\n",
      "Episode [4300/10000]\tAverage Shortfall: $647,308.59\n",
      "Episode [4400/10000]\tAverage Shortfall: $679,268.08\n",
      "Episode [4500/10000]\tAverage Shortfall: $684,543.19\n",
      "Episode [4600/10000]\tAverage Shortfall: $681,547.57\n",
      "Episode [4700/10000]\tAverage Shortfall: $658,825.98\n",
      "Episode [4800/10000]\tAverage Shortfall: $708,013.93\n",
      "Episode [4900/10000]\tAverage Shortfall: $696,404.67\n",
      "Episode [5000/10000]\tAverage Shortfall: $546,905.68\n",
      "Episode [5100/10000]\tAverage Shortfall: $623,116.56\n",
      "Episode [5200/10000]\tAverage Shortfall: $656,624.56\n",
      "Episode [5300/10000]\tAverage Shortfall: $643,298.86\n",
      "Episode [5400/10000]\tAverage Shortfall: $647,271.86\n",
      "Episode [5500/10000]\tAverage Shortfall: $686,763.08\n",
      "Episode [5600/10000]\tAverage Shortfall: $635,564.95\n",
      "Episode [5700/10000]\tAverage Shortfall: $710,733.67\n",
      "Episode [5800/10000]\tAverage Shortfall: $714,081.95\n",
      "Episode [5900/10000]\tAverage Shortfall: $690,433.05\n",
      "Episode [6000/10000]\tAverage Shortfall: $650,066.51\n",
      "Episode [6100/10000]\tAverage Shortfall: $648,723.50\n",
      "Episode [6200/10000]\tAverage Shortfall: $654,647.71\n",
      "Episode [6300/10000]\tAverage Shortfall: $595,881.37\n",
      "Episode [6400/10000]\tAverage Shortfall: $641,737.57\n",
      "Episode [6500/10000]\tAverage Shortfall: $704,275.96\n",
      "Episode [6600/10000]\tAverage Shortfall: $624,863.08\n",
      "Episode [6700/10000]\tAverage Shortfall: $767,171.23\n",
      "Episode [6800/10000]\tAverage Shortfall: $667,569.08\n",
      "Episode [6900/10000]\tAverage Shortfall: $669,944.76\n",
      "Episode [7000/10000]\tAverage Shortfall: $643,312.42\n",
      "Episode [7100/10000]\tAverage Shortfall: $668,247.29\n",
      "Episode [7200/10000]\tAverage Shortfall: $623,285.64\n",
      "Episode [7300/10000]\tAverage Shortfall: $677,378.20\n",
      "Episode [7400/10000]\tAverage Shortfall: $714,435.00\n",
      "Episode [7500/10000]\tAverage Shortfall: $681,034.04\n",
      "Episode [7600/10000]\tAverage Shortfall: $656,972.81\n",
      "Episode [7700/10000]\tAverage Shortfall: $659,253.01\n",
      "Episode [7800/10000]\tAverage Shortfall: $651,387.87\n",
      "Episode [7900/10000]\tAverage Shortfall: $703,574.91\n",
      "Episode [8000/10000]\tAverage Shortfall: $732,012.45\n",
      "Episode [8100/10000]\tAverage Shortfall: $604,609.78\n",
      "Episode [8200/10000]\tAverage Shortfall: $718,823.25\n",
      "Episode [8300/10000]\tAverage Shortfall: $691,818.85\n",
      "Episode [8400/10000]\tAverage Shortfall: $640,463.42\n",
      "Episode [8500/10000]\tAverage Shortfall: $666,899.22\n",
      "Episode [8600/10000]\tAverage Shortfall: $714,990.84\n",
      "Episode [8700/10000]\tAverage Shortfall: $692,086.39\n",
      "Episode [8800/10000]\tAverage Shortfall: $698,208.41\n",
      "Episode [8900/10000]\tAverage Shortfall: $686,082.83\n",
      "Episode [9000/10000]\tAverage Shortfall: $667,459.79\n",
      "Episode [9100/10000]\tAverage Shortfall: $619,056.31\n",
      "Episode [9200/10000]\tAverage Shortfall: $731,176.63\n",
      "Episode [9300/10000]\tAverage Shortfall: $672,173.79\n",
      "Episode [9400/10000]\tAverage Shortfall: $638,313.64\n",
      "Episode [9500/10000]\tAverage Shortfall: $699,920.95\n",
      "Episode [9600/10000]\tAverage Shortfall: $700,549.29\n",
      "Episode [9700/10000]\tAverage Shortfall: $664,039.76\n",
      "Episode [9800/10000]\tAverage Shortfall: $670,872.60\n",
      "Episode [9900/10000]\tAverage Shortfall: $645,644.73\n",
      "Episode [10000/10000]\tAverage Shortfall: $624,531.79\n",
      "\n",
      "Average Implementation Shortfall: $670,327.17 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "\n",
    "# Create simulation environment\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with reward function $$ r = \\frac{r_{bar} - \\gamma p}{M} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "\n",
    "# Create simulation environment\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with reward as r based on almgren chriss utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100/10000]\tAverage Shortfall: $1,114,855.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m new_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# current state, action, reward, new state are stored in the experience replay\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# roll over new state\u001b[39;00m\n\u001b[0;32m     48\u001b[0m cur_state \u001b[38;5;241m=\u001b[39m new_state\n",
      "File \u001b[1;32mc:\\Users\\itspa\\Downloads\\Personal Projects\\newtone-bootcamp-final-project\\financial\\finance\\ddpg_agent.py:62\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[0;32m     61\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\itspa\\Downloads\\Personal Projects\\newtone-bootcamp-final-project\\financial\\finance\\ddpg_agent.py:105\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Minimize the loss\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 105\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# ---------------------------- update actor ---------------------------- #\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\itspa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\itspa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\itspa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "\n",
    "# Create simulation environment\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with reward being $$ r = Q_t(P - P_0) - \\alpha d_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100/10000]\tAverage Shortfall: $1,114,855.70\n",
      "Episode [200/10000]\tAverage Shortfall: $736,189.16\n",
      "Episode [300/10000]\tAverage Shortfall: $653,020.01\n",
      "Episode [400/10000]\tAverage Shortfall: $657,455.83\n",
      "Episode [500/10000]\tAverage Shortfall: $713,015.27\n",
      "Episode [600/10000]\tAverage Shortfall: $642,479.18\n",
      "Episode [700/10000]\tAverage Shortfall: $662,310.81\n",
      "Episode [800/10000]\tAverage Shortfall: $669,257.31\n",
      "Episode [900/10000]\tAverage Shortfall: $671,740.87\n",
      "Episode [1000/10000]\tAverage Shortfall: $659,986.13\n",
      "Episode [1100/10000]\tAverage Shortfall: $663,747.48\n",
      "Episode [1200/10000]\tAverage Shortfall: $666,158.99\n",
      "Episode [1300/10000]\tAverage Shortfall: $621,277.47\n",
      "Episode [1400/10000]\tAverage Shortfall: $643,741.79\n",
      "Episode [1500/10000]\tAverage Shortfall: $680,184.38\n",
      "Episode [1600/10000]\tAverage Shortfall: $685,300.58\n",
      "Episode [1700/10000]\tAverage Shortfall: $721,398.55\n",
      "Episode [1800/10000]\tAverage Shortfall: $674,768.55\n",
      "Episode [1900/10000]\tAverage Shortfall: $671,332.48\n",
      "Episode [2000/10000]\tAverage Shortfall: $683,757.23\n",
      "Episode [2100/10000]\tAverage Shortfall: $608,460.90\n",
      "Episode [2200/10000]\tAverage Shortfall: $630,664.00\n",
      "Episode [2300/10000]\tAverage Shortfall: $663,729.88\n",
      "Episode [2400/10000]\tAverage Shortfall: $671,909.57\n",
      "Episode [2500/10000]\tAverage Shortfall: $591,556.20\n",
      "Episode [2600/10000]\tAverage Shortfall: $701,376.68\n",
      "Episode [2700/10000]\tAverage Shortfall: $663,724.92\n",
      "Episode [2800/10000]\tAverage Shortfall: $655,136.50\n",
      "Episode [2900/10000]\tAverage Shortfall: $659,788.76\n",
      "Episode [3000/10000]\tAverage Shortfall: $646,975.91\n",
      "Episode [3100/10000]\tAverage Shortfall: $640,154.93\n",
      "Episode [3200/10000]\tAverage Shortfall: $650,621.20\n",
      "Episode [3300/10000]\tAverage Shortfall: $694,887.65\n",
      "Episode [3400/10000]\tAverage Shortfall: $721,790.54\n",
      "Episode [3500/10000]\tAverage Shortfall: $623,096.97\n",
      "Episode [3600/10000]\tAverage Shortfall: $663,852.82\n",
      "Episode [3700/10000]\tAverage Shortfall: $636,976.71\n",
      "Episode [3800/10000]\tAverage Shortfall: $585,254.08\n",
      "Episode [3900/10000]\tAverage Shortfall: $616,676.65\n",
      "Episode [4000/10000]\tAverage Shortfall: $615,902.16\n",
      "Episode [4100/10000]\tAverage Shortfall: $718,621.12\n",
      "Episode [4200/10000]\tAverage Shortfall: $684,708.62\n",
      "Episode [4300/10000]\tAverage Shortfall: $647,308.59\n",
      "Episode [4400/10000]\tAverage Shortfall: $679,268.08\n",
      "Episode [4500/10000]\tAverage Shortfall: $684,543.19\n",
      "Episode [4600/10000]\tAverage Shortfall: $681,547.57\n",
      "Episode [4700/10000]\tAverage Shortfall: $658,825.98\n",
      "Episode [4800/10000]\tAverage Shortfall: $708,013.93\n",
      "Episode [4900/10000]\tAverage Shortfall: $696,404.67\n",
      "Episode [5000/10000]\tAverage Shortfall: $546,905.68\n",
      "Episode [5100/10000]\tAverage Shortfall: $623,116.56\n",
      "Episode [5200/10000]\tAverage Shortfall: $656,624.56\n",
      "Episode [5300/10000]\tAverage Shortfall: $643,298.86\n",
      "Episode [5400/10000]\tAverage Shortfall: $647,271.86\n",
      "Episode [5500/10000]\tAverage Shortfall: $686,763.08\n",
      "Episode [5600/10000]\tAverage Shortfall: $635,564.95\n",
      "Episode [5700/10000]\tAverage Shortfall: $710,733.67\n",
      "Episode [5800/10000]\tAverage Shortfall: $714,081.95\n",
      "Episode [5900/10000]\tAverage Shortfall: $690,433.05\n",
      "Episode [6000/10000]\tAverage Shortfall: $650,066.51\n",
      "Episode [6100/10000]\tAverage Shortfall: $648,723.50\n",
      "Episode [6200/10000]\tAverage Shortfall: $654,647.71\n",
      "Episode [6300/10000]\tAverage Shortfall: $595,881.37\n",
      "Episode [6400/10000]\tAverage Shortfall: $641,737.57\n",
      "Episode [6500/10000]\tAverage Shortfall: $704,275.96\n",
      "Episode [6600/10000]\tAverage Shortfall: $624,863.08\n",
      "Episode [6700/10000]\tAverage Shortfall: $767,171.23\n",
      "Episode [6800/10000]\tAverage Shortfall: $667,569.08\n",
      "Episode [6900/10000]\tAverage Shortfall: $669,944.76\n",
      "Episode [7000/10000]\tAverage Shortfall: $643,312.42\n",
      "Episode [7100/10000]\tAverage Shortfall: $668,247.29\n",
      "Episode [7200/10000]\tAverage Shortfall: $623,285.64\n",
      "Episode [7300/10000]\tAverage Shortfall: $677,378.20\n",
      "Episode [7400/10000]\tAverage Shortfall: $714,435.00\n",
      "Episode [7500/10000]\tAverage Shortfall: $681,034.04\n",
      "Episode [7600/10000]\tAverage Shortfall: $656,972.81\n",
      "Episode [7700/10000]\tAverage Shortfall: $659,253.01\n",
      "Episode [7800/10000]\tAverage Shortfall: $651,387.87\n",
      "Episode [7900/10000]\tAverage Shortfall: $703,574.91\n",
      "Episode [8000/10000]\tAverage Shortfall: $732,012.45\n",
      "Episode [8100/10000]\tAverage Shortfall: $604,609.78\n",
      "Episode [8200/10000]\tAverage Shortfall: $718,823.25\n",
      "Episode [8300/10000]\tAverage Shortfall: $691,818.85\n",
      "Episode [8400/10000]\tAverage Shortfall: $640,463.42\n",
      "Episode [8500/10000]\tAverage Shortfall: $666,899.22\n",
      "Episode [8600/10000]\tAverage Shortfall: $714,990.84\n",
      "Episode [8700/10000]\tAverage Shortfall: $692,086.39\n",
      "Episode [8800/10000]\tAverage Shortfall: $698,208.41\n",
      "Episode [8900/10000]\tAverage Shortfall: $686,082.83\n",
      "Episode [9000/10000]\tAverage Shortfall: $667,459.79\n",
      "Episode [9100/10000]\tAverage Shortfall: $619,056.31\n",
      "Episode [9200/10000]\tAverage Shortfall: $731,176.63\n",
      "Episode [9300/10000]\tAverage Shortfall: $672,173.79\n",
      "Episode [9400/10000]\tAverage Shortfall: $638,313.64\n",
      "Episode [9500/10000]\tAverage Shortfall: $699,920.95\n",
      "Episode [9600/10000]\tAverage Shortfall: $700,549.29\n",
      "Episode [9700/10000]\tAverage Shortfall: $664,039.76\n",
      "Episode [9800/10000]\tAverage Shortfall: $670,872.60\n",
      "Episode [9900/10000]\tAverage Shortfall: $645,644.73\n",
      "Episode [10000/10000]\tAverage Shortfall: $624,531.79\n",
      "\n",
      "Average Implementation Shortfall: $670,320.47 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "\n",
    "# Create simulation environment\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100/10000]\tAverage Shortfall: $1,114,855.70\n",
      "Episode [200/10000]\tAverage Shortfall: $736,189.16\n",
      "Episode [300/10000]\tAverage Shortfall: $653,020.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m new_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# current state, action, reward, new state are stored in the experience replay\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# roll over new state\u001b[39;00m\n\u001b[0;32m     51\u001b[0m cur_state \u001b[38;5;241m=\u001b[39m new_state\n",
      "File \u001b[1;32mc:\\Users\\itspa\\Downloads\\Personal Projects\\newtone-bootcamp-final-project\\financial\\finance\\ddpg_agent.py:66\u001b[0m, in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, add_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\itspa\\Downloads\\Personal Projects\\newtone-bootcamp-final-project\\financial\\finance\\ddpg_agent.py:183\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    182\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39maction \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 183\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    184\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mnext_state \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    185\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\itspa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "import ddpg_agent\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "importlib.reload(ddpg_agent)\n",
    "\n",
    "\n",
    "# Create simulation environment\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import importlib\n",
    "import syntheticChrissAlmgren as sca\n",
    "import ddpg_agent\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "importlib.reload(sca)\n",
    "importlib.reload(ddpg_agent)\n",
    "\n",
    "importlib.reload(sca)\n",
    "importlib.reload(ddpg_agent)\n",
    "\n",
    "test_env = sca.MarketEnvironment()\n",
    "agent = Agent(state_size=test_env.observation_space_dimension(), action_size=test_env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, episodes=100):\n",
    "    shortfalls = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset(seed=10000 + ep)\n",
    "        env.start_transactions()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, = agent.act(state, add_noise=False)\n",
    "            state, _, done, info = env.step(action)\n",
    "        shortfalls.append(info.implementation_shortfall)\n",
    "    return np.array(shortfalls)\n",
    "\n",
    "def evaluate_ac(env, episodes=100):\n",
    "    frac_schedule = env.get_trade_list() / env.total_shares  \n",
    "    shortfalls = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset(seed=20000 + ep)\n",
    "        env.start_transactions()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            action = frac_schedule[t]  \n",
    "            state, _, done, info = env.step(action)\n",
    "            t += 1\n",
    "        shortfalls.append(info.implementation_shortfall)\n",
    "    return np.array(shortfalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sfs = evaluate_agent(test_env, agent, episodes=1000)\n",
    "ac_sfs    = evaluate_ac(test_env, episodes=1000)\n",
    "\n",
    "print(\"Agent: mean=${:.2f}, std=${:.2f}\".format(agent_sfs.mean(), agent_sfs.std()))\n",
    "print(\"AC   : mean=${:.2f}, std=${:.2f}\".format(ac_sfs.mean(),    ac_sfs.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "The above code should provide you with a starting framework for incorporating more complex dynamics into our model. Here are a few things you can try out:\n",
    "\n",
    "- Explain why log-returns in a time window of 6 periods, along with $m_k$ and $i_k$ is a good choice for the state? Could you expand or shrink $D$ = number of past log-returns (which is considered $D=5$) to get better results?\n",
    "\n",
    "- Incorporate your own reward function in the simulation environmet to see if you can achieve a expected shortfall that is better (lower) than that produced by the Almgren and Chriss model.\n",
    "\n",
    "\n",
    "- Experiment rewarding the agent at every step and only giving a reward at the end. Which is; what happens if the reward function is sparse?\n",
    "\n",
    "\n",
    "- Use more realistic price dynamics, such as geometric brownian motion (GBM). The equations used to model GBM can be found in section 3b of paper: GBM\n",
    "\n",
    "\n",
    "- Try different functions for the action. You can change the values of the actions produced by the agent by using different functions. You can choose your function depending on the interpretation you give to the action. For example, you could set the action to be a **function of the trading rate**.\n",
    "\n",
    "\n",
    "- Add more complex dynamics to the environment. Try incorporate trading fees, for example. This can be done by adding and extra term to the fixed cost of selling, $\\epsilon$.\n",
    "\n",
    "- Use SAC (soft actor-critic) and TD3 (Twin Delayed Deep Deterministic) with different hyperparameters and network structures to compare your results to DDPG results. Explain why this happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
